# -*- coding: utf-8 -*-
"""Car-GenuineSynthetic-Final-CapstoneZ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14LFBkbaDKfdSI2IQYnvvXPmEmh2-I14d
"""

#my imports
import numpy as np
import unicodedata
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
import string
from nltk import pos_tag
import pandas as pd
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer
pd.set_option('display.max_colwidth', -1)
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
import random

#Github Links 
#Test Data
negative_car_test = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/negative_car_test.csv')
positive_car_test = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/positive_car_test.csv')
total_car_test = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/total_car_test-2.csv')
#Minimum Data
negative_car_minimum = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/negative_car_minimum.csv')
positive_car_minimum = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/positive_car_minimum.csv')
total_car_minimum = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/total_car_minimum.csv')
#Remainder Data
negative_car_remainder = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/negative_car_remainder.csv')
positive_car_remainder = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/positive_car_remainder.csv')
total_car_remainder = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/total_car_remainder.csv')
#Total Training Data
master_negative_car_training = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/master_negative_car_training_final.csv')
master_positive_car_training = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/master_positive_car_training_final.csv')
total_master_training_final = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/total_master_training_final.csv')



#Assigning Negative/Positive to copy
negative_car_text = master_negative_car_training.copy()
positive_car_text = master_positive_car_training.copy()

#Positive and Negative Series for analysis
negative_car_series = negative_car_text["text"]
positive_car_series = positive_car_text["text"]

#Making negative training series
#negative_car_text.drop(["rating", "Unnamed: 0"], axis = 1, inplace=True)
#positive_car_text.drop(["rating", "Unnamed: 0"], axis = 1, inplace=True)

#negative_car_text.to_csv('negative_car.txt')

#positive_car_text.to_csv('positive_car.txt')



#This is the code to get the text from GPT-2 to a Dataframe. neg_car_synthetic_df
"""
#POSITIVE DATAFRAME
#COPY THE GPT FILE WITH DELIMETERS OF " ^ " 
#RUN THE FILE THROUGH THIS THE FOLLOWING FUNCTION
#https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/positive_car_working_final.txt
def file_string():
    with open("/content/positive_car_working_final.txt",'r') as file:
        lines = file.read()

    return lines.replace('-\n','').replace('\n', ' ') 

new_file = (file_string())

#Split the file
big_break = new_file.split('^')

#Create the lists to combine into a dataframe
stars = []
rating = []

#Create the columns
for x in big_break:
  stars.append(2)
  rating.append("positive")

#Make the dataframe
my_dict = {"rating" : rating, 'text': big_break}
pos_car_synthetic_df = pd.DataFrame(my_dict)
"""

#This is the code to get the text from GPT-2 to a Dataframe. neg_car_synthetic_df
"""
#NEGATIVE DATAFRAME
#COPY THE GPT FILE WITH DELIMETERS OF " ^ " 
#RUN THE FILE THROUGH THIS THE FOLLOWING FUNCTION
#https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/negative_car_final_working.txt
def file_string():
    with open("/content/negative_car_final_working.txt",'r') as file:
        lines = file.read()

    return lines.replace('-\n','').replace('\n', ' ') 

new_file = (file_string())

#Split the file
big_break = new_file.split('^')

#Create the lists to combine into a dataframe
stars = []
rating = []

#Create the columns
for x in big_break:
  stars.append(2)
  rating.append("negative")

#Make the dataframe
my_dict = {"rating" : rating, 'text': big_break}
neg_car_synthetic_df = pd.DataFrame(my_dict)
"""

#Negative Synthetic Data
neg_car_synthetic_df = pd.read_csv("https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/capstone_synthetic_neg_car.csv")

#Positive Synthetic Data
pos_car_synthetic_df = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/capstone_synthetic_pos_car.csv')

#neg_car_synthetic_df.to_csv('capstone_synthetic_neg_car.csv')

#Shuffling dataframe
neg_car_synthetic_df = neg_car_synthetic_df.sample(frac=1).reset_index(drop=True)
pos_car_synthetic_df = pos_car_synthetic_df.sample(frac=1).reset_index(drop=True)

#Synthetic Data Broken Down
#Pos_car = 6008
#neg_car = 6044
a_neg = neg_car_synthetic_df[1:500]
a_pos = pos_car_synthetic_df[1:500]
b_neg = neg_car_synthetic_df[501:1000]
b_pos = pos_car_synthetic_df[501:1000]
c_neg = neg_car_synthetic_df[1001:1500]
c_pos = pos_car_synthetic_df[1001:1500]
d_neg = neg_car_synthetic_df[1501:2000]
d_pos = pos_car_synthetic_df[1501:2000]
e_neg = neg_car_synthetic_df[2001:2500]
e_pos = pos_car_synthetic_df[2001:2500]
f_neg = neg_car_synthetic_df[2501:3000]
f_pos = pos_car_synthetic_df[2501:3000]
g_neg = neg_car_synthetic_df[3001:3500]
g_pos = pos_car_synthetic_df[3001:3500]
h_neg = neg_car_synthetic_df[3501:4000]
h_pos = pos_car_synthetic_df[3501:4000]
i_neg = neg_car_synthetic_df[4001:4500]
i_pos = pos_car_synthetic_df[4001:4500]
j_neg = neg_car_synthetic_df[4501:5000]
j_pos = pos_car_synthetic_df[4501:5000]
k_neg = neg_car_synthetic_df[5001:5500]
k_pos = pos_car_synthetic_df[5001:5500]
l_neg = neg_car_synthetic_df[5501:6008]
l_pos = pos_car_synthetic_df[5501:6008]
#2 thousand synthetic data
a_two_thousand = pd.concat([a_neg,b_neg,c_neg,d_neg,a_pos,b_pos,c_pos,d_pos], axis = 0).reset_index(drop=True)
b_two_thousand = pd.concat([e_neg,f_neg,g_neg,h_neg,e_pos,f_pos,g_pos,h_pos], axis = 0).reset_index(drop=True)
c_two_thousand = pd.concat([i_neg,j_neg,k_neg,l_neg,i_pos,j_pos,k_pos,l_pos], axis = 0).reset_index(drop=True)
#Full Synthetic Data
all_synthetic_data = pd.concat([neg_car_synthetic_df, pos_car_synthetic_df], axis = 0).reset_index(drop=True)

#Important Dataframes

#Test Data Set
total_car_test = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/total_car_test-2.csv')

#Genuine Data Set Full
total_master_training_final = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/total_master_training_final.csv')

#Minimum Data Set
total_car_minimum = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/total_car_minimum.csv')

#test variables
new_x_train, new_x_test, new_y_train, new_y_test = train_test_split(total_car_test['text'], total_car_test['rating'], random_state = 0, test_size = .991)

#GPT ONLY Double (Random)
a_two_thousand = pd.concat([a_neg,b_neg,c_neg,d_neg,a_pos,b_pos,c_pos,d_pos], axis = 0).reset_index(drop=True)
four_thousand_gpt = pd.concat([a_two_thousand, b_two_thousand], axis=0).reset_index(drop=True)
six_thousand_gpt = pd.concat([a_two_thousand, b_two_thousand, c_two_thousand], axis=0).reset_index(drop=True)

#Full Genuine and Full Synthetic
synth_gen_full = pd.concat([total_master_training_final, all_synthetic_data], axis = 0).reset_index(drop=True)
synth_gen_full.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Full Genuine with Level 1 Synthetic(2000)
lv_one_gen_synth = pd.concat([total_master_training_final, a_two_thousand], axis = 0).reset_index(drop=True)
lv_one_gen_synth.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Full Genuine with Level 2 Synthetic(4000)
lv_two_gen_synth = pd.concat([total_master_training_final, four_thousand_gpt], axis = 0).reset_index(drop=True)
lv_two_gen_synth.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Minimum Genuine and Full Synthetic
synth_gen_minimum = pd.concat([all_synthetic_data, total_car_minimum], axis = 0).reset_index(drop=True)
synth_gen_minimum.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Minimum Data
negative_car_minimum = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/negative_car_minimum.csv')
positive_car_minimum = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/positive_car_minimum.csv')
total_car_minimum = pd.read_csv('https://raw.githubusercontent.com/success81/Synthetic_NLP_Data_Generation_Paper/main/Car_repair/total_car_minimum.csv')

#Lite Genuine
lite_neg = negative_car_minimum[1:51]
lite_pos = positive_car_minimum[1:51]
total_lite = pd.concat([lite_neg,lite_pos], axis = 0).reset_index(drop=True)
total_lite.drop(["Unnamed: 0"], axis = 1, inplace=True)

#GPT ONLY
a_two_thousand = pd.concat([a_neg,b_neg,c_neg,d_neg,a_pos,b_pos,c_pos,d_pos], axis = 0).reset_index(drop=True)
b_two_thousand = pd.concat([e_neg,f_neg,g_neg,h_neg,e_pos,f_pos,g_pos,h_pos], axis = 0).reset_index(drop=True)
c_two_thousand = pd.concat([i_neg,j_neg,k_neg,l_neg,i_pos,j_pos,k_pos,l_pos], axis = 0).reset_index(drop=True)

#GPT ONLY Double
a_two_thousand = pd.concat([a_neg,b_neg,c_neg,d_neg,a_pos,b_pos,c_pos,d_pos], axis = 0).reset_index(drop=True)
four_thousand_gpt = pd.concat([a_two_thousand, b_two_thousand], axis=0).reset_index(drop=True)
six_thousand_gpt = pd.concat([a_two_thousand, b_two_thousand, c_two_thousand], axis=0).reset_index(drop=True)


#GPT LITE MIX

two_thousand_lite = pd.concat([a_two_thousand, total_lite], axis=0).reset_index(drop=True)
four_thousand_lite = pd.concat([a_two_thousand, b_two_thousand, total_lite], axis=0).reset_index(drop=True)
six_thousand_lite = pd.concat([a_two_thousand, b_two_thousand, c_two_thousand, total_lite], axis=0).reset_index(drop=True)

#Range of Cars Dataset
#Car 1000
scale_lv_1_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos], axis=0).reset_index(drop=True)
scale_lv_1_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 2000
scale_lv_2_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos], axis=0).reset_index(drop=True)
scale_lv_2_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 3000
scale_lv_3_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos], axis=0).reset_index(drop=True)
scale_lv_3_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 4000
scale_lv_4_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos], axis=0).reset_index(drop=True)
scale_lv_4_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 5000
scale_lv_5_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos, e_neg, e_pos], axis=0).reset_index(drop=True)
scale_lv_5_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 6000
scale_lv_6_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos, e_neg, e_pos, f_neg, f_pos], axis=0).reset_index(drop=True)
scale_lv_6_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 7000
scale_lv_7_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos, e_neg, e_pos, f_neg, f_pos, g_neg, g_pos], axis=0).reset_index(drop=True)
scale_lv_7_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 8000
scale_lv_8_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos, e_neg, e_pos, f_neg, f_pos, g_neg, g_pos, h_pos, h_neg], axis=0).reset_index(drop=True)
scale_lv_8_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 9000
scale_lv_9_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos, e_neg, e_pos, f_neg, f_pos, g_neg, g_pos, h_pos, h_neg, i_pos, i_neg], axis=0).reset_index(drop=True)
scale_lv_9_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 10000
scale_lv_10_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos, e_neg, e_pos, f_neg, f_pos, g_neg, g_pos, h_pos, h_neg,i_pos, i_neg, j_neg, j_pos], axis=0).reset_index(drop=True)
scale_lv_10_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 11000
scale_lv_11_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos, e_neg, e_pos, f_neg, f_pos, g_neg, g_pos, h_pos, h_neg,i_pos, i_neg, j_neg, j_pos, k_neg, k_pos], axis=0).reset_index(drop=True)
scale_lv_11_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

#Car 12000
scale_lv_12_gen_syn = pd.concat([total_master_training_final, a_neg, a_pos, b_neg, b_pos, c_neg, c_pos, d_neg, d_pos, e_neg, e_pos, f_neg, f_pos, g_neg, g_pos, h_pos, h_neg,i_pos, i_neg, j_neg, j_pos, k_neg, k_pos, l_neg, l_pos], axis=0).reset_index(drop=True)
scale_lv_12_gen_syn.drop(["Unnamed: 0"], axis = 1, inplace=True)

# check for missing values
display(synth_gen_full.isna().any())
 
# drop any missing values
synth_gen_full = synth_gen_full.dropna()

"""### ***Model Testing Bayes***"""

#This is the Training of the Genuine and Synthetic Naive Bayes Model For Car Reviews
x_train, x_test, y_train, y_test = train_test_split(synth_gen_full['text'], synth_gen_full['rating'], random_state = 0)
count_vect = CountVectorizer()
x_train_counts = count_vect.fit_transform(x_train)
tfidf_transformer = TfidfTransformer()
x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)
clf = MultinomialNB().fit(x_train_tfidf, y_train)

#New Predict
naive_bayes_predict = clf.predict(count_vect.transform(new_x_test))

#Precision Score
precision_score(new_y_test, naive_bayes_predict, average="weighted")

#accuracy_score
accuracy_score(new_y_test, naive_bayes_predict)

#Recall Score
recall_score(new_y_test, naive_bayes_predict, average="weighted")

#F1
f1_score(new_y_test, naive_bayes_predict, average="weighted")

#Confusion Matrix
cm = confusion_matrix(new_y_test, naive_bayes_predict)
print (cm)

#Scores for Range chart 4 features. I am adding 1000 synthetic datapoints to the genuine data

"""
Level 0 NO SYNTHETIC DATA
-Precision: 0.8202310510002817
-Accuracy: 0.717948717948718
-Recall: 0.717948717948718
-F1: 0.6944314535313351

Level 1 1000 synthetic data boost
-Precision: 0.8682246024018175
-Accuracy: 0.8205128205128205
-Recall: 0.8205128205128205
-F1: 0.8148401126051814

Level 2 2000 synthetic data boost
-Precision: 0.8776778776778777
-Accuracy: 0.8376068376068376
-Recall: 0.8376068376068376
-F1: 0.8334513223402112

Level 3 3000 synthetic data boost
-Precision: 0.8876353276353276
-Accuracy: 0.8547008547008547
-Recall: 0.8547008547008547
-F1: 0.8517587444111675


Level 4 4000 synthetic data boost
-Precision: 0.8876353276353276
-Accuracy: 0.8547008547008547
-Recall: 0.8547008547008547
-F1: 0.8517587444111675

Level 5 5000 synthetic data boost
-Precision: 0.9092331768388107
-Accuracy: 0.8888888888888888
-Recall: 0.8888888888888888
-F1: 0.8876092038882736

Level 6 6000 synthetic data boost
-Precision: 0.8981383912890762
-Accuracy: 0.8888888888888888
-Recall: 0.8888888888888888
-F1: 0.8876092038882736

Level 7 7000 synthetic data boost
-Precision: 0.8912886460345126
-Accuracy: 0.8717948717948718
-Recall: 0.8717948717948718
-F1: 0.8703183121787773

Level 8 8000 synthetic data boost
-Precision: 0.9032809983896939
-Accuracy: 0.8888888888888888
-Recall: 0.8888888888888888
-F1: 0.8879894686061439

Level 9 9000 synthetic data boost
-Precision: 0.9036087369420703
-Accuracy: 0.8803418803418803
-Recall: 0.8803418803418803
-F1: 0.8787310979618672

Level 10 10000 synthetic data boost
-Precision: 0.8876353276353276
-Accuracy: 0.8547008547008547
-Recall: 0.8547008547008547
-F1: 0.8517587444111675

Level 11 11000 synthetic data boost
-Precision: 0.915018315018315
-Accuracy: 0.8974358974358975
-Recall: 0.8974358974358975
-F1: 0.8964380745041122

Level 12 12000 synthetic data boost
-Precision: 0.9032809983896939
-Accuracy: 0.8888888888888888
-Recall: 0.8888888888888888
-F1: 0.8879894686061439
"""

"""
Scores
#####THis is the experiment showing an 80< model getting data added to it
Genuine only
-Precision: 0.8202310510002817
-Accuaracy: 0.717948717948718
-Recall: 0.717948717948718
-F1: 0.6944314535313351

Genuine with Level 1 (2000 sythetic)
-Precision: 0.9036087369420703
-Accuracy: 0.8803418803418803
-Recall: 0.8803418803418803
-F1: 0.8787310979618672

Genuine with Level 2 (4000 Synthetic)
-Precision: 0.9036087369420703
-Accuracy: 0.8803418803418803
-Recall: 0.8803418803418803
-F1: 0.8787310979618672



Genuine with Full Synthetic
-Precision: 0.9036087369420703
-Accuaracy: 0.8803418803418803
-Recall: 0.8803418803418803
-F1: 0.8787310979618672

#####This is an alternative experiment showing limited data and synthetic
Genuine only minimum dataset
-Precision: 0.8101472995090017
-Accuracy: 0.6923076923076923
-Recall: 0.6923076923076923
-F1: 0.6612027253875776

Genuine Minimum with Synthetic
-Precision: 0.915018315018315
-Accuracy: 0.8974358974358975
-Recall: 0.8974358974358975
-F1: 0.8964380745041122

#####This is an experiment to show super limited data gradually augmented with Synthetic Data
Lite (50 observations)
-Precision: 0.788948125581789
-Accuracy: 0.6324786324786325
-Recall: 0.6324786324786325
-F1: 0.5768187926678493

Synthetic Alone Level 1
-Precision: 0.8912886460345126
-Accuracy: 0.8717948717948718
-Recall: 0.8717948717948718
-F1: 0.8703183121787773

Synthetic Alone Level 2
-Precision: 0.8912886460345126
-Accuracy: 0.8717948717948718
-Recall: 0.8717948717948718
-F1: 0.8703183121787773

Synthetic Alone Level 3
-Precision: 0.8855908584169455
-Accuracy: 0.8717948717948718
-Recall: 0.8717948717948718
-F1: 0.8703183121787773

Synthetic / Mix Level 1
-Precision: 0.8912886460345126
-Accuracy: 0.8717948717948718
-Recall: 0.8717948717948718
-F1: 0.8703183121787773

Synthetic / Mix Level 2
-Precision: 0.8912886460345126
-Accuracy: 0.8717948717948718
-Recall: 0.8717948717948718
-F1: 0.8703183121787773

Synthetic / Mix Level 3
-Precision: 0.8855908584169455
-Accuracy: 0.8717948717948718
-Recall: 0.8717948717948718
-F1: 0.8707570791609354

"""